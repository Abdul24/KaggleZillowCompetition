{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Table of Contents\n",
    "   \n",
    "1. Introduction\n",
    "2. Import and Merge Data \n",
    "3. Data Exploratory Analysis and Visualization\n",
    "3. Feature Engineering\n",
    "4. Machine Learning\n",
    "5. Closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Use the real estate datasets from Zillow competition on Kaggle website to do data exploration and analyses\n",
    "The feature/predictor datasets are related with different aspect of some real estate sold in 2016 and 2017\n",
    "The response datasets are the log error difference between zestimate and real sold prices\n",
    "Because I started this project when the submission deadline for this stage of the Kaggle competition already passed, I will not be able to attend the competition, yet this project is still a good practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data handling/modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import scipy.stats as stats\n",
    "\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")  \n",
    "import matplotlib as mpl\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import 2016 and 2017 Date and Log Error Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read 2016 property file\n",
    "train2016_df = pd.read_csv(\"../Data/train_2016_v2.csv\")\n",
    "train2017_df = pd.read_csv(\"../Data/train_2017.csv\")\n",
    "display(train2016_df.columns)\n",
    "display(train2017_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train2016_df.shape)\n",
    "display(train2017_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train2016_df.dtypes)\n",
    "display(train2017_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train2016_df.head())\n",
    "display(train2017_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Date Time Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2016_df[\"TransDate\"] = pd.to_datetime(train2016_df.transactiondate)\n",
    "train2016_df['YearMonth'] = train2016_df['TransDate'].map(lambda x: 100*x.year + x.month)\n",
    "train2016_df[\"TransYear\"] = train2016_df[\"TransDate\"].dt.year\n",
    "train2016_df[\"TransMonth\"] = train2016_df[\"TransDate\"].dt.month\n",
    "train2016_df.drop('transactiondate', axis=1, inplace=True)\n",
    "display(train2016_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2017_df[\"TransDate\"] = pd.to_datetime(train2017_df.transactiondate)\n",
    "train2017_df['YearMonth'] = train2017_df['TransDate'].map(lambda x: 100*x.year + x.month)\n",
    "train2017_df[\"TransYear\"] = train2017_df[\"TransDate\"].dt.year\n",
    "train2017_df[\"TransMonth\"] = train2017_df[\"TransDate\"].dt.month\n",
    "train2017_df.drop('transactiondate', axis=1, inplace=True)\n",
    "display(train2017_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import 2016 and 2017 Feature Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2016_df = pd.read_csv(\"../Data/properties_2016.csv\")\n",
    "data2016_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2017_df = pd.read_csv(\"../Data/properties_2017.csv\")\n",
    "data2017_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data2016_df.shape)\n",
    "display(data2017_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_df = data2016_df.iloc[:,[22,32,34,49,55]]\n",
    "problem_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the four columns of data that caused warning not only have mixed data types, but also have either some data that I couldn't translate or have too many NaN data, therefore I will just drop these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2016_df.drop(data2016_df.columns[[22,32,34,49,55]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2017_df.drop(data2017_df.columns[[22,32,34,49,55]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data2016_df.shape)\n",
    "display(data2017_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Dataframes Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full2016_df = pd.merge(train2016_df, data2016_df, how = 'inner', on = ['parcelid'])\n",
    "full2017_df = pd.merge(train2017_df, data2017_df, how = 'inner', on = ['parcelid'])\n",
    "display(full2016_df.shape)\n",
    "display(full2017_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full2016_df.append(full2017_df)\n",
    "full_df = full_df.set_index([list(range(0,167888))])\n",
    "display(full_df.shape)\n",
    "display(full_df.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploratory Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,15))\n",
    "sns.heatmap(full_df.corr())\n",
    "#print (full2016_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_correlations = full_df.corr()\n",
    "data_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_No = full_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_ratio = null_No/full_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nullRatio = pd.DataFrame(null_ratio,columns=[\"Null Ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nullRatio.sort_values(\"Null Ratio\",ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullRatio.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_name = list(nullRatio.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pos = np.arange(len(col_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,18))\n",
    "plt.barh(y_pos, nullRatio['Null Ratio'], align='center', color='red')\n",
    "plt.yticks(y_pos, col_name);\n",
    "ax.set_xlabel(\"Percentage of missing values\")\n",
    "ax.set_title(\"Percentage of missing values in each column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few predictors have similar names. Plot them out and see their relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(full_df, x_vars=[\"roomcnt\"], y_vars=['calculatedbathnbr'], size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(full_df, x_vars=[\"TransYear\"], y_vars=['assessmentyear'], size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(full_df, x_vars=[\"finishedsquarefeet12\"], y_vars=['calculatedfinishedsquarefeet'], size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of logerror change with both quantitative and qualitative variables \n",
    "sns.pairplot(full_df, x_vars=[\"yearbuilt\"], y_vars=[\"logerror\"], hue='heatingorsystemtypeid', size=6, palette='husl')\n",
    "sns.pairplot(full_df, x_vars=[\"TransMonth\"], y_vars=[\"logerror\"], hue='propertylandusetypeid', size=6, palette='husl')\n",
    "sns.pairplot(full_df, x_vars=['lotsizesquarefeet'], y_vars=['logerror'], hue='buildingqualitytypeid', size=6, palette='husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different room count impact on log error\n",
    "sns.pairplot(full_df, x_vars=[\"bedroomcnt\",\"bathroomcnt\",\"roomcnt\",\"calculatedbathnbr\"], y_vars=[\"logerror\"], size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tax value impact on log error\n",
    "sns.pairplot(full_df, x_vars=[\"taxvaluedollarcnt\",\"landtaxvaluedollarcnt\",\"taxamount\"], y_vars=[\"logerror\"], size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analyses above, key features are selected and divided into three groups: numeric features, catgorical features and location related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_names = ['TransYear','TransMonth','yearbuilt','calculatedfinishedsquarefeet','lotsizesquarefeet','calculatedbathnbr']\n",
    "cat_names = ['propertylandusetypeid','buildingqualitytypeid','heatingorsystemtypeid']\n",
    "loc_names = ['latitude','longitude','regionidzip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.regionidzip.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.regionidzip.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in Null Values\n",
    "null values are filled in differently according to their different types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in numeric columns according to their median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the numer of missing values per column\n",
    "num_col = full_df[num_names]\n",
    "print(\"\\nNumber of missing values per column\")\n",
    "print(num_col.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the median value for each column\n",
    "median_per_column = full_df[num_names].apply(lambda x: x.median(),axis=0)\n",
    "print (\"Median value per column:\\n\",median_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in with median values\n",
    "num_df = full_df[num_names].fillna(median_per_column,axis=0)\n",
    "print(\"numer of missing values after filling in\")\n",
    "display(num_df.isnull().sum())\n",
    "display(num_df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in categorical columns using their most frequent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the numer of missing values per column\n",
    "cat_col = full_df[cat_names]\n",
    "print(\"\\nNumber of missing values per column\")\n",
    "print(cat_col.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the most frequent number for each column\n",
    "def get_most_frequent_value(my_column):\n",
    "    return my_column.value_counts().index[0]\n",
    "\n",
    "most_frequent_values_per_column = full_df[cat_names].apply(get_most_frequent_value,axis=0)\n",
    "print (\"Most frequent value in each column:\\n\",most_frequent_values_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in with most frequent values\n",
    "cat_df = full_df[cat_names].fillna(most_frequent_values_per_column,axis=0)\n",
    "cat_df = cat_df.astype(int) \n",
    "print(\"numer of missing values after filling in\")\n",
    "display(cat_df.isnull().sum())\n",
    "display(cat_df.tail(10))\n",
    "cat_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in location columns with 0 to replace NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the numer of missing values per column\n",
    "loc_col = full_df[loc_names]\n",
    "print(\"\\nNumber of missing values per column\")\n",
    "print(loc_col.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in with 0\n",
    "loc_df = full_df[loc_names].fillna(0,axis=0)\n",
    "print(\"numer of missing values after filling in\")\n",
    "display(loc_df.isnull().sum())\n",
    "display(loc_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Property land use type ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First look at its distribution\n",
    "LandType = cat_df.propertylandusetypeid.value_counts()\n",
    "LandType.plot(kind='bar', title=\"Property Land Type Counts\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlabel(\"Property Land Type\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "form four variables for the property land type according to the above types:\n",
    "1. SFR: Single Family Residential (261)\n",
    "2. CON: Condominium (266)\n",
    "3. DUP: Duplex (2 Units, Any Combination) (246)\n",
    "4. PUD: Planned Unit Development (269)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate these new feature columns\n",
    "col = ['SFR','CON','DUP','PUD']\n",
    "typ = [261,266,246,269]\n",
    "for a in range(4):\n",
    "    cat_df[col[a]] = cat_df.propertylandusetypeid == typ[a]\n",
    "    cat_df[col[a]] = cat_df[col[a]].astype(int)\n",
    "cat_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building quality type ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QualityType = cat_df.buildingqualitytypeid.value_counts()\n",
    "QualityType.plot(kind='bar', title=\"Quality Type Counts\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlabel(\"Quality Type\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form four variables for the quality type according to their distributions:\n",
    "1. Q7: 7\n",
    "2. Q4: 4\n",
    "3. Q1: 1\n",
    "4. Q10: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate these new feature columns\n",
    "col = ['Q7','Q4','Q1','Q10']\n",
    "typ = [7,4,1,10]\n",
    "for a in range(4):\n",
    "    cat_df[col[a]] = cat_df.buildingqualitytypeid == typ[a]\n",
    "    cat_df[col[a]] = cat_df[col[a]].astype(int)\n",
    "cat_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heating system type ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeatType = cat_df.heatingorsystemtypeid.value_counts()\n",
    "HeatType.plot(kind='bar', title=\"Heat System Type Counts\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlabel(\"Heat System Type\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate these new feature columns\n",
    "col = ['Central','FW','Yes','FAir']\n",
    "typ = [2,7,24,6]\n",
    "for a in range(4):\n",
    "    cat_df[col[a]] = cat_df.heatingorsystemtypeid == typ[a]\n",
    "    cat_df[col[a]] = cat_df[col[a]].astype(int)\n",
    "cat_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCat_df = cat_df.iloc[:,3:]\n",
    "display(newCat_df.tail(5))\n",
    "newCat_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler() #create a scaler object\n",
    "scaler.fit(num_df) #fit the scaler\n",
    "num_scaled = scaler.transform(num_df) #transform the data with it\n",
    "newNum_df = pd.DataFrame(num_scaled,columns=num_names)\n",
    "display(newNum_df.tail(5))\n",
    "newNum_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Item: Analyze and minimize skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature columns after engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = pd.concat((newNum_df,newCat_df),axis=1)\n",
    "display(feat_df.head(5))\n",
    "feat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name response column\n",
    "loger_df = full_df['logerror']\n",
    "display(loger_df.head(5))\n",
    "loger_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use two methods to fit data individually and then use the relative more accurate one for data prediction\n",
    "1. Method One: Regression -- interpretable fitting results\n",
    "2. Method Two: Random Forrest -- more flexible, possibly more accurate fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Data Using Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Train Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(feat_df, loger_df, test_size=0.4, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on training set\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "#generate predictions on training set and evaluate\n",
    "y_pred_train = linreg.predict(X_train)\n",
    "print( \"Training set RMSE:\",np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "#generate predictions on test set and evaluate\n",
    "y_pred_test = linreg.predict(X_test)\n",
    "print( \"Test set RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Model with Second Order Interaction Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features seem to interact with each other (for instance, a house with large sqft is also more likely to have a large yard) therefore I would like to also include second order interaction variables into the inputs to see if that can result in a better model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the interactions of numeric features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_fit_2 = PolynomialFeatures(degree=2,interaction_only=True)\n",
    "X_train_ = poly_fit_2.fit_transform(X_train)\n",
    "X_test_ = poly_fit_2.fit_transform(X_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on training set\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_, y_train)\n",
    "\n",
    "#generate predictions on training set and evaluate\n",
    "y_pred_train_ = linreg.predict(X_train_)\n",
    "print( \"Training set RMSE:\",np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_)))\n",
    "\n",
    "#generate predictions on test set and evaluate\n",
    "y_pred_test_ = linreg.predict(X_test_)\n",
    "print( \"Test set RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Cross Validation\n",
    "mean_squared_errors = np.abs(cross_val_score(linreg,feat_df,loger_df,cv=50,scoring='neg_mean_squared_error'))\n",
    "root_mean_squared_errors = list(map(np.sqrt,mean_squared_errors))\n",
    "print (\"50-fold mean RMSE: \", np.mean(root_mean_squared_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Data Using Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the same set of features, yet without numeric values scaled\n",
    "feat_df2 = pd.concat((num_df,newCat_df),axis=1)\n",
    "X_train2,X_test2,y_train2,y_test2 = train_test_split(feat_df2, loger_df, test_size=0.4, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfreg = RandomForestRegressor(n_estimators=200, max_features=18, bootstrap=True, oob_score=True, random_state=123)\n",
    "rfreg.fit(X_train2,y_train2)\n",
    "preds = rfreg.predict(X_test2)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test2,preds))\n",
    "print(\"RMSE:\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the same train test split datasets from the regression model (with numeric features scaled too)\n",
    "rfreg = RandomForestRegressor(n_estimators=200, max_features=18, bootstrap=True, oob_score=True, random_state=12)\n",
    "rfreg.fit(X_train,y_train)\n",
    "preds = rfreg.predict(X_test)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test,preds))\n",
    "print(\"RMSE:\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_names = list(feat_df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importances\n",
    "pd.DataFrame({'feature':feat_names, \n",
    "              'importance':rfreg.feature_importances_}).sort_values(by='importance',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was not able to finish everything planned in order to build a better model.\n",
    "Other things that I planned to do for this project include:\n",
    "1. Add other important features into the model, such as all the region related features; try a few more other features such as basementsqft, garagetotalsqft and update features with the best selections\n",
    "2. Also drop the not-so-relevant features that I've chosen by examing the P values to see if they make statistical difference; and try to use PCA\n",
    "3. Try to optimize the parameters used in Random Forrest Model such as n_estimators and max_features\n",
    "4. Try other methods and algorithms that may bring better fitting and predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
